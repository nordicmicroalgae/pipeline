---
title: "Update Nordic Microalgae Taxonomy"
author: "Anders Torstensson, SMHI"
date: "`r Sys.Date()`"
output: html_document
params: 
  cache_update_checklist: FALSE # Run on cached data, FALSE if doing annual updates
  cache_taxa_worms: FALSE # Run on cached data, FALSE if doing annual updates
  cache_synonyms: FALSE # Run on cached data, FALSE if doing annual updates
  cache_dyntaxa: FALSE # Run on cached data, FALSE if doing annual updates
  cache_algaebase: FALSE # Run on cached data, FALSE if doing annual updates
  cache_norcca: FALSE # Run on cached data, FALSE if doing annual updates
  cache_itis: FALSE # Run on cached data, FALSE if doing annual updates
  cache_ncbi: FALSE # Run on cached data, FALSE if doing annual updates
  cache_gbif: FALSE # Run on cached data, FALSE if doing annual updates
knit: (function(inputFile, encoding) {
                        rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file=paste0("update_history/update_report_",
                        format(Sys.Date(), "%Y%m%d"),
                        ".html")) })
---

This Rmarkdown file calls necessary R and Python code for updating the taxonomic backbone for Nordic Microalgae, through various APIs and webpage interactions. Links to required Python code is provided below. Records can be cached when running the script multiple times. Remember to set to cache == FALSE in params before data are prepared for NuA content. Current [NOMP Biovolume file](http://nordicmicroalgae.org/tools) and [HAB IOC list](https://www.marinespecies.org/hab/aphia.php?p=download&what=taxlist) needs to be downloaded separately and placed in data_in/.

* Additional taxa can be included in additions_to_old_nua.txt
* Unwanted taxa, e.g. Flagellates, can be included in blacklist.txt
* The inclusion of an unaccepted can be forced by adding the taxa to whitelist.txt

# Setup
Valid subscription keys from [Dyntaxa](https://api-portal.artdatabanken.se/) and [AlgaeBase](https://www.algaebase.org/) are required to match Dyntaxa and AlgaeBase records. The keys can be stored in .Renviron or typed in when running the code.

```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

knit.time <- Sys.time()

# Load required libraries
library(reticulate)
library(snakecase)
library(worrms)
library(writexl)
library(readxl)
library(SHARK4R)
library(zip)
library(algaeClassify)
library(rgbif)
library(yaml)
library(rvest)
library(tidyverse)

# Load API keys from .Renviron
subscription_key <- Sys.getenv("DYNTAXA_APIKEY")
ALGAEBASE_APIKEY <- Sys.getenv("ALGAEBASE_APIKEY")

# Print if records are cached
if(subscription_key == "") {
  subscription_key <- rstudioapi::askForPassword(prompt = "Please enter your SLU Artdatabanken API key")
}

# Print if records are cached
if(ALGAEBASE_APIKEY == "") {
  ALGAEBASE_APIKEY <- rstudioapi::askForPassword(prompt = "Please enter your AlgaeBase API key")
}

# Define cache types
cache_types <- c("update_checklist", "taxa_worms", "synonyms", "dyntaxa", "algaebase", "norcca", "itis", "ncbi", "gbif")

# Iterate over cache types
for (cache_type in cache_types) {
  if (params[[paste0("cache_", cache_type)]]) {
    print(paste("Warning:", toupper(cache_type), "records are loaded from CACHE"))
  }
}
```

# Get current aphia ids for taxa list (NOMP + old checklist + Karlson HAB list)

```{r update_checklist, include=FALSE}
if(!params$cache_update_checklist) {
  file.remove("cache/all_records_cache.rda")
}

start.time <- Sys.time()
source("src/R/01_get_current_aphia_ids.R")
end.time <- Sys.time()
runtime_update_checklist <- round(end.time - start.time, 2)
```

```{r checklist_result, echo=FALSE}
print(paste("AphiaID for", nrow(all_records), "taxa found"))
```

# Create taxa_worms.txt, including higher taxonomy
using https://github.com/nordicmicroalgae/taxa-worms

```{r get_taxa_file, include=FALSE}
if(!params$cache_taxa_worms) {
  file.remove("cache/worms_cache.db")
}

start.time <- Sys.time()
wormsextractor <- import_from_path("wormsextractor", path = "src/python/")

taxa_list_generator <- wormsextractor$TaxaListGenerator()

taxa_list_generator$run_all()
end.time <- Sys.time()
runtime_get_taxa_file <- round(end.time - start.time, 2)
```

# Find unaccepted and duplicated taxa
Unaccepted taxa are introduced when getting higher taxonomy in the previous chunk and are removed in this step

```{r filter_taxa_worms, include=FALSE}
source("src/R/02_filter_taxa_worms.R")
```

```{r unaccepted_taxa, echo=FALSE}
# Print table
taxa_worms_unaccepted %>%
  select(-classification,
         -url,
         -genus,
         -family,
         -order,
         -class,
         -phylum,
         -kingdom) %>%
  arrange(scientific_name) %>%
  knitr::kable(caption = "Unaccepted taxa removed from taxa.txt")
```

# Find and remove duplicates from taxa file
Potential duplicate errors could be removed in blacklist

```{r duplicate_taxa, echo=FALSE}
# Print table
duplicates %>%
  select(-classification,
         -url,
         -genus,
         -family,
         -order,
         -class,
         -phylum,
         -kingdom) %>%
  knitr::kable(caption = "Duplicate taxa names")

# Print
print(paste(nrow(taxa_worms_accepted), "taxa exported to taxa.txt"))
```

# Get all synonyms from WoRMS

```{r get_synonyms, include=FALSE}
if(!params$cache_synonyms) {
  file.remove("cache/synonyms_cache.rda")
}

start.time <- Sys.time()
source("src/R/03_get_worms_synonyms.R")
end.time <- Sys.time()
runtime_synonyms <- round(end.time - start.time, 2)

# Print output
print(paste(length(worms_synonyms$synonym_name), 
            "synonyms found for", 
            length(unique(worms_synonyms$taxon_id)),
            "taxa"))
```

# Match taxa with Dyntaxa

```{r get_dyntaxa, echo=FALSE, message=FALSE}
if(!params$cache_dyntaxa) {
  file.remove("cache/dyntaxa_cache.rda")
}

start.time <- Sys.time()
source("src/R/04_get_dyntaxa_links.R")
end.time <- Sys.time()
runtime_match_dyntaxa <- round(end.time - start.time, 2)
```

# Export files to send to AlgaeBase

```{r get_algaebase, echo=FALSE, include=FALSE}
if(!params$cache_algaebase) {
  file.remove("cache/algaebase_cache.rda")
}

start.time <- Sys.time()
source("src/R/05_get_algaebase_links.R")
end.time <- Sys.time()
runtime_algaebase <- round(end.time - start.time, 2)
```

```{r summary_algaebase, echo=FALSE}
# Print
print(paste(nrow(algaebase_results), "species and genera found in AlgaeBase"))
```

# Extract culture information from NORCCA 
using https://github.com/nordicmicroalgae/norcca_compiler

```{r get_norcca, include=FALSE, message=FALSE, warning=FALSE}
start.time <- Sys.time()

norcca_compiler <- import_from_path("norcca_compiler", path = "src/python/")

strains_compiler <- norcca_compiler$StrainsCompiler()

strains_compiler$compile()

norcca <- do.call(rbind, lapply(strains_compiler$rows, 
                                function(x) as.data.frame(t(unlist(x)))))

write_delim(norcca, "data_in/norcca_strains.txt", 
            delim = "\t",
            na = "")

print(paste(nrow(norcca), "strains extracted"))
```

```{r wrangle_norcca, echo=FALSE}
if(!params$cache_norcca) {
  file.remove("cache/norcca_cache.rda")
}

source("src/R/06_match_norcca_names.R")
end.time <- Sys.time()
runtime_get_norcca <- round(end.time - start.time, 2)
```

# Reformat IOC HAB list and add taxon_id
Downloaded (Text file, tab delimited) from
https://www.marinespecies.org/hab/aphia.php?p=download&what=taxlist

```{r get_hab, echo=FALSE, include=FALSE}
source("src/R/07_get_hab_links.R")
```

```{r summary_hab, echo=FALSE}
# Print
print(paste(nrow(nordic_hab), "IOC HAB species found in database"))
```

# Get ITIS links
```{r get_itis, echo=FALSE, include=FALSE}
if(!params$cache_itis) {
  file.remove("cache/itis_cache.rda")
}

start.time <- Sys.time()
source("src/R/08_get_itis_links.R")
end.time <- Sys.time()
runtime_itis <- round(end.time - start.time, 2)
```

```{r summary_itis, echo=FALSE}
# Print
print(paste(nrow(itis_list), "taxa found in ITIS database"))
```

# Get NCBI and ENA links
```{r get_ncbi, echo=FALSE, include=FALSE}
if(!params$cache_ncbi) {
  file.remove("cache/ncbi_cache.rda")
}

start.time <- Sys.time()
source("src/R/09_get_ncbi_links.R")
end.time <- Sys.time()
runtime_ncbi <- round(end.time - start.time, 2)
```

```{r summary_ncbi, echo=FALSE}
# Print
print(paste(nrow(ncbi_list), "taxa found in NCBI/ENA database"))
```

# Get GBIF links
```{r get_gbif, echo=FALSE, include=FALSE}
if(!params$cache_gbif) {
  file.remove("cache/gbif_cache.rda")
}

start.time <- Sys.time()
source("src/R/10_get_gbif_links.R")
end.time <- Sys.time()
runtime_gbif <- round(end.time - start.time, 2)

runtime_knit <- round(end.time - knit.time, 2)
```

```{r summary_gbif, echo=FALSE}
# Print
print(paste(nrow(gbif_list), "taxa found in GBIF"))
```

# Update filters.yaml for backend
Filters for Quick View may need to be updated
```{r update_filters, echo=FALSE}
source("src/R/11_update_filters.R")
```

# Summarise runtimes
```{r api_operation_summary, echo=FALSE}
print(paste("Time taken for running the entire script:"))
print(runtime_knit)

print(paste("Chunk time taken for updating checklist:"))
print(runtime_update_checklist)

print(paste("Chunk time taken for getting taxa.txt information:"))
print(runtime_get_taxa_file)

print(paste("Chunk time taken for getting WoRMS synonyms:"))
print(runtime_synonyms)

print(paste("Chunk time taken for getting Dyntaxa links:"))
print(runtime_match_dyntaxa)

print(paste("Chunk time taken for getting AlgaeBase links:"))
print(runtime_algaebase)

print(paste("Chunk time taken for getting NORCCA links:"))
print(runtime_get_norcca)

print(paste("Chunk time taken for getting ITIS links:"))
print(runtime_itis)

print(paste("Chunk time taken for getting NCBI links:"))
print(runtime_ncbi)

print(paste("Chunk time taken for getting GBIF links:"))
print(runtime_gbif)
```

# Create zip-archive
Store all output in update_history/update_archive_`r format(knit.time, "%Y%m%d")`.zip

```{r update_archive, echo=FALSE}
zip::zip(zipfile = paste0('update_history/update_archive_', 
                          format(knit.time, "%Y%m%d"),
                          ".zip"
), 
files = c('data_out/backend/taxa/config/filters.yaml',
          'data_out/content/facts_biovolumes_nomp.txt',
          'data_out/content/facts_external_links_algaebase.txt',
          'data_out/content/facts_external_links_dyntaxa.txt',
          'data_out/content/facts_external_links_ena.txt',
          'data_out/content/facts_external_links_gbif.txt',
          'data_out/content/facts_external_links_hab_ioc.txt',
          'data_out/content/facts_external_links_itis.txt',
          'data_out/content/facts_external_links_ncbi.txt',
          'data_out/content/facts_external_links_norcca.txt',
          'data_out/content/facts_external_links_worms.txt',
          'data_out/content/facts_hab_ioc.txt',
          'data_out/content/synonyms.txt',
          'data_out/content/taxa.txt',
          'data_out/duplicated_scientific_name.txt',
          'data_out/taxa_worms_unaccepted.txt',
          'data_out/taxa_worms.txt',
          'data_out/translate_to_worms.txt',
          paste0("data_out/nordicmicroalgae_checklist_", date, ".txt"),
          paste0("data_out/nordicmicroalgae_checklist_", date, ".xlsx")
)
)
```


# Reproducibility

```{r reproducibility, echo=FALSE}
# Date time
Sys.time()
# Here we store the session info for this script
sessioninfo::session_info()
```