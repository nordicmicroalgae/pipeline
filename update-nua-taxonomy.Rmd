---
title: "Update Nordic Microalgae Taxonomy"
author: "Anders Torstensson, SMHI"
date: "`r Sys.Date()`"
output: html_document
params: 
  cache_update_checklist: FALSE # Run on cached data, FALSE if doing annual updates
  cache_taxa_worms: FALSE # Run on cached data, FALSE if doing annual updates
  cache_synonyms: FALSE # Run on cached data, FALSE if doing annual updates
  cache_algaebase: FALSE # Run on cached data, FALSE if doing annual updates
  cache_dyntaxa: FALSE # Run on cached data, FALSE if doing annual updates
knit: (function(inputFile, encoding) {
                        rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file=paste0("update_history/update_report_",
                        format(Sys.Date(), "%Y%m%d"),
                        ".html")) })
---

This Rmarkdown file calls necessary R and Python code for updating the taxonomic backbone for Nordic Microalgae, through various APIs and webpage interactions. Links to required Python code is provided below. Records can be cached when running the script multiple times. Remember to set to cache == FALSE in params before data are prepared for NuA content. Current [NOMP Biovolume file](http://nordicmicroalgae.org/tools) and [HAB IOC list](https://www.marinespecies.org/hab/aphia.php?p=download&what=taxlist) needs to be downloaded separately and placed in data_in/.

# Setup
A valid subscription key from https://api-portal.artdatabanken.se/ is required to match Dyntaxa records. The key can be stored in file or typed in when running the code.

```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load required libraries
library(reticulate)
library(snakecase)
library(worrms)
library(writexl)
library(SHARK4R)
library(tidyverse)
library(zip)
library(algaeClassify)

subscription_key <- Sys.getenv("DYNTAXA_APIKEY")
ALGAEBASE_APIKEY <- Sys.getenv("ALGAEBASE_APIKEY")

# Print if records are cached
if(subscription_key == "") {
  subscription_key <- rstudioapi::askForPassword(prompt = "Please enter your SLU Artdatabanken API key")
}

# Print if records are cached
if(ALGAEBASE_APIKEY == "") {
  ALGAEBASE_APIKEY <- rstudioapi::askForPassword(prompt = "Please enter your AlgaeBase API key")
}

if(params$cache_update_checklist) {
  print(paste("Warning: update_checklist records are loaded from CACHE"))
}

if(params$cache_taxa_worms) {
  print(paste("Warning: taxa_worms records are loaded from CACHE"))
}

if(params$cache_synonyms) {
  print(paste("Warning: synonyms are loaded from CACHE"))
}

if(params$cache_algaebase) {
  print(paste("Warning: AlgaeBase records are loaded from CACHE"))
}

if(params$cache_dyntaxa) {
  print(paste("Warning: dyntaxa records are loaded from CACHE"))
}
```

# Get current aphia ids for taxa list (NOMP + old checklist)

```{r update_checklist, include=FALSE}
if(!params$cache_update_checklist) {
  file.remove("all_records_cache.rda")
}

start.time <- Sys.time()
source("code/01_get_current_aphia_ids.R")
end.time <- Sys.time()
runtime_update_checklist <- round(end.time - start.time, 2)
```

```{r checklist_result, echo=FALSE}
print(paste("AphiaID for", nrow(all_records), "taxa found"))
```

# Create taxa_worms.txt, including higher taxonomy
using https://github.com/nordicmicroalgae/taxa-worms

```{r get_taxa_file, include=FALSE}
if(!params$cache_taxa_worms) {
  file.remove("worms_cache.db")
}

start.time <- Sys.time()
py_run_file("../taxa-worms/extract_from_worms_main.py")
end.time <- Sys.time()
runtime_get_taxa_file <- round(end.time - start.time, 2)
```

# Get all synonyms from WoRMS and find unaccepted and duplicated taxa
Unaccepted taxa are introduced when getting higher taxonomy in the previous chunk and are removed in this step

```{r synonyms, include=FALSE}
if(!params$cache_synonyms) {
  file.remove("synonyms_cache.rda")
}

start.time <- Sys.time()
source("code/02_get_worms_synonyms.R")
end.time <- Sys.time()
runtime_synonyms <- round(end.time - start.time, 2)
```

```{r unaccepted_taxa, echo=FALSE}
# Print output
print(paste(length(worms_synonyms$synonym_name), 
            "synonyms found for", 
            length(unique(worms_synonyms$taxon_id)),
            "taxa"))

# Print table
taxa_worms_unaccepted %>%
  select(-classification,
         -url,
         -genus,
         -family,
         -order,
         -class,
         -phylum,
         -kingdom) %>%
  arrange(scientific_name) %>%
  knitr::kable(caption = "Unaccepted taxa removed from taxa.txt")
```

# Find and remove duplicates from taxa file manually
Potential duplicate errors could be deleted in this chunk

```{r clean_duplicate_taxa, echo=FALSE}
# Print table
duplicates %>%
  select(-classification,
         -url,
         -genus,
         -family,
         -order,
         -class,
         -phylum,
         -kingdom) %>%
  knitr::kable(caption = "Duplicates before cleanup")

# Select taxon_id to remove, e.g. same name for multiple taxonomic levels
taxa_remove <- c(599656, # Glaucophyta Phylum (Division)
                 582161) # Euglenozoa Infrakingdom

# Print
print(paste("Removing", length(taxa_remove), "duplicated taxa from taxa.txt"))

# Read taxa_worms file and remove unwanted taxa
taxa_worms <- read_tsv("data_out/content/taxa.txt",
                       col_types = cols()) %>%
  filter(!taxon_id %in% taxa_remove)

# Find duplicated taxa names
duplicates <- taxa_worms %>%
  filter(duplicated(scientific_name))

# Print table
duplicates %>%
  select(-classification,
         -url,
         -genus,
         -family,
         -order,
         -class,
         -phylum,
         -kingdom) %>%
  knitr::kable(caption = "Duplicates after cleanup")

# Print
print(paste(nrow(taxa_worms), "taxa exported to taxa.txt"))

# Store cleaned file
write_tsv(taxa_worms, "data_out/content/taxa.txt", na = "") 
```

# Match taxa with Dyntaxa

```{r match_dyntaxa, echo=FALSE, message=FALSE}
if(!params$cache_dyntaxa) {
  file.remove("dyntaxa_cache.rda")
}

start.time <- Sys.time()
source("code/03_match_dyntaxa.R")
end.time <- Sys.time()
runtime_match_dyntaxa <- round(end.time - start.time, 2)
```

# Export files to send to AlgaeBase

```{r export_algaebase, echo=FALSE, include=FALSE}
if(!params$cache_algaebase) {
  file.remove("algaebase_cache.rda")
}

start.time <- Sys.time()
source("code/04_export_algaebase.R")
end.time <- Sys.time()
runtime_algaebase <- round(end.time - start.time, 2)
```

```{r summary_algaebase, echo=FALSE}
# Print
print(paste(nrow(algaebase_results), "species and genera found in AlgaeBase"))
```

# Extract culture information from NORCCA 
using https://github.com/nordicmicroalgae/norcca_compiler

```{r get_norcca, include=FALSE, message=FALSE, warning=FALSE}
start.time <- Sys.time()

setwd("../norcca_compiler")

system("venv\\Scripts\\activate")

system("pip install beautifulsoup4")
system("pip install requests")

system("python -m norcca_compiler --output norcca_strains.txt")

norcca <- read_tsv("norcca_strains.txt")

setwd(here::here())

write_delim(norcca, "data_in/norcca_strains.txt", 
            delim = "\t",
            na = "")

print(paste(nrow(norcca), "strains extracted"))

end.time <- Sys.time()
runtime_get_norcca <- round(end.time - start.time, 2)
```

```{r wrangle_norcca, echo=FALSE}
source("code/05_wrangle_norcca.R")
```

# Reformat IOC HAB list and add taxon_id
Downloaded (Text file, tab delimited) from
https://www.marinespecies.org/hab/aphia.php?p=download&what=taxlist

```{r wrangle_hab_list, echo=FALSE}
source("code/06_wrangle_hab.R")
```

# Summarise runtimes
```{r api_operation_summary, echo=FALSE}
print(paste("Time taken for updating checklist:"))
print(runtime_update_checklist)

print(paste("Time taken for getting taxa.txt information:"))
print(runtime_get_taxa_file)

print(paste("Time taken for getting WoRMS synonyms:"))
print(runtime_synonyms)

print(paste("Time taken for getting Dyntaxa links:"))
print(runtime_match_dyntaxa)

print(paste("Time taken for getting AlgaeBase links:"))
print(runtime_algaebase)

print(paste("Time taken for getting NORCCA links:"))
print(runtime_get_norcca)
```

# Create zip-archive
Store all output in update_history/update_archive_YYYYMMDD.zip

```{r update_archive, echo=FALSE}
zip::zip(zipfile = paste0('update_history/update_archive_', 
                     format(Sys.Date(), "%Y%m%d"),
                     ".zip"
                     ), 
    files = c('data_out/content/facts_biovolumes_nomp.txt',
              'data_out/content/facts_external_links_dyntaxa.txt',
              'data_out/content/facts_external_links_hab_ioc.txt',
              'data_out/content/facts_external_links_norcca.txt',
              'data_out/content/facts_external_links_worms.txt',
              'data_out/content/synonyms.txt',
              'data_out/content/taxa.txt',
              'data_out/nordic_microalgae_genus.xlsx',
              'data_out/nordic_microalgae_higher_taxonomy.xlsx',
              'data_out/nordic_microalgae_species.xlsx',
              'data_out/duplicated_scientific_name.txt',
              'data_out/taxa_worms_unaccepted.txt',
              'data_out/taxa_worms.txt',
              'data_out/translate_to_worms.txt',
              paste0("data_out/nordicmicroalgae_checklist_", date, ".txt"),
              paste0("data_out/nordicmicroalgae_checklist_", date, ".xlsx")
              )
    )
```


# Reproducibility

```{r reproducibility, echo=FALSE}
# Date time
Sys.time()
# Here we store the session info for this script
sessioninfo::session_info()
```