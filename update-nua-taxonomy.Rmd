---
title: "Nordic Microalgae Data Pipeline"
author: "Anders Torstensson, SMHI"
date: "`r Sys.Date()`"
output: html_document
params: 
  cache_update_checklist: FALSE
  cache_taxa_worms: FALSE
  cache_synonyms: FALSE
  cache_dyntaxa: FALSE
  cache_algaebase: FALSE
  cache_norcca: FALSE
  cache_itis: FALSE
  cache_ncbi: FALSE
  cache_gbif: FALSE
  cache_shark: FALSE
knit: (function(inputFile, encoding) {
                        rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file=paste0("update_history/update_report_",
                        format(Sys.Date(), "%Y%m%d"),
                        ".html")) })
---

This Rmarkdown file calls necessary R and Python code for gathering the necessary data for updating the taxonomic backbone for Nordic Microalgae, through various APIs and webpage interactions. Records can be cached when running the script multiple times. Remember to set to cache == FALSE in params before data are prepared for an anual update of the NuA species content content. Current [NOMP Biovolume file](http://nordicmicroalgae.org/tools) and [HAB IOC list](https://www.marinespecies.org/hab/aphia.php?p=download&what=taxlist) needs to be downloaded separately and placed in data_in/.

* Additional taxa can be included in additions_to_old_nua.txt
* Unwanted taxa, e.g. Flagellates, can be included in blacklist.txt
* The inclusion of unaccepted taxa can be forced by adding the AphiaID to whitelist.txt

# Setup
Valid subscription keys from [Dyntaxa](https://api-portal.artdatabanken.se/) and [AlgaeBase](https://www.algaebase.org/) are required to match Dyntaxa and AlgaeBase records. The keys can be stored in .Renviron or typed in when running the setup chunk.

```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

knit.time <- Sys.time()

# Load required libraries
library(reticulate)
library(snakecase)
library(worrms)
library(writexl)
library(readxl)
library(SHARK4R)
library(zip)
library(algaeClassify)
library(rgbif)
library(yaml)
library(rvest)
library(tidyverse)

# Setup Python virtual environment
virtualenv_create("src/python/venv", requirements = "src/python/requirements.txt")
use_virtualenv("src/python/venv")

# Load API keys from .Renviron
subscription_key <- Sys.getenv("DYNTAXA_APIKEY")
ALGAEBASE_APIKEY <- Sys.getenv("ALGAEBASE_APIKEY")

# Prompt for API key if not available in env
if(subscription_key == "") {
  subscription_key <- rstudioapi::askForPassword(prompt = "Please enter your SLU Artdatabanken API key")
  Sys.setenv(DYNTAXA_APIKEY = subscription_key)
}

# Prompt for API key if not available in env
if(ALGAEBASE_APIKEY == "") {
  ALGAEBASE_APIKEY <- rstudioapi::askForPassword(prompt = "Please enter your AlgaeBase API key")
  Sys.setenv(ALGAEBASE_APIKEY = ALGAEBASE_APIKEY)
}

# Define cache types
cache_types <- c("update_checklist", "taxa_worms", "synonyms", "dyntaxa", "algaebase", "norcca", "itis", "ncbi", "gbif", "shark")

# Iterate over cache types
for (cache_type in cache_types) {
  if (params[[paste0("cache_", cache_type)]]) {
    cat("Warning:", toupper(cache_type), "records are set to load from CACHE\n")
  }
}
```

# Get current aphia ids for taxa list (NOMP + old checklist + Karlson HAB list + manual additions)

```{r update_checklist, include=FALSE}
if(!params$cache_update_checklist & file.exists("cache/all_records_cache.rda")) {
  temp <- file.remove("cache/all_records_cache.rda")
}

start.time <- Sys.time()
source("src/R/01_get_current_aphia_ids.R")
end.time <- Sys.time()
runtime_update_checklist <- round(end.time - start.time, 2)
```

```{r checklist_result, echo=FALSE}
cat("AphiaID for", nrow(all_records), "taxa found")
```

# Get WoRMS records, including higher taxonomy

```{r get_taxa_file, include=FALSE}
if(!params$cache_taxa_worms & file.exists("cache/worms_cache.rda")) {
  temp <- file.remove("cache/worms_cache.db")
}

start.time <- Sys.time()
wormsextractor <- import_from_path("wormsextractor", path = "src/python/")

taxa_list_generator <- wormsextractor$TaxaListGenerator()

taxa_list_generator$run_all()
end.time <- Sys.time()
runtime_get_taxa_file <- round(end.time - start.time, 2)
```

# Find unaccepted and duplicated taxa
Unaccepted taxa are introduced when getting higher taxonomy in the previous chunk and are removed in this step. Parents are redirected to their closest relative.

```{r filter_taxa_worms, include=FALSE}
source("src/R/02_filter_taxa_worms.R")
```

```{r unaccepted_taxa, echo=FALSE}
# Print
cat(nrow(taxa_worms_accepted), "taxa exported to taxa.txt after filtration")

# Print table
taxa_worms_unaccepted %>%
  select(-classification,
         -url,
         -genus,
         -family,
         -order,
         -class,
         -phylum,
         -kingdom) %>%
  arrange(scientific_name) %>%
  knitr::kable(caption = "Unaccepted taxa removed from taxa.txt")
```

# Find and remove duplicates from taxa file
Potential duplicate errors could be removed in blacklist

```{r duplicate_taxa, echo=FALSE}
# Print table
duplicates %>%
  select(-classification,
         -url,
         -genus,
         -family,
         -order,
         -class,
         -phylum,
         -kingdom) %>%
  knitr::kable(caption = "Duplicate taxa names")
```

# Get all synonyms from WoRMS

```{r get_synonyms, include=FALSE}
if(!params$cache_synonyms & file.exists("cache/synonyms_cache.rda")) {
  temp <- file.remove("cache/synonyms_cache.rda")
}

start.time <- Sys.time()
source("src/R/03_get_worms_synonyms.R")
end.time <- Sys.time()
runtime_synonyms <- round(end.time - start.time, 2)
```

```{r summary_synonyms, echo=FALSE}
# Print output
cat(length(worms_synonyms$synonym_name), 
    "synonyms found for", 
    length(unique(worms_synonyms$taxon_id)),
    "taxa")
```

# Match taxa with Dyntaxa

```{r get_dyntaxa, echo=FALSE, message=FALSE}
if(!params$cache_dyntaxa & file.exists("cache/dyntaxa_cache.rda")) {
  temp <- file.remove("cache/dyntaxa_cache.rda")
}

start.time <- Sys.time()
source("src/R/04_get_dyntaxa_links.R")

# Print output
cat(length(unique(dyntaxa_list$taxon_id)),
    "taxa found in Dyntaxa")

end.time <- Sys.time()
runtime_match_dyntaxa <- round(end.time - start.time, 2)
```

# Get AlgaeBase links

```{r get_algaebase, echo=FALSE, include=FALSE}
if(!params$cache_algaebase & file.exists("cache/algaebase_cache.rda")) {
  temp <- file.remove("cache/algaebase_cache.rda")
}

start.time <- Sys.time()
source("src/R/05_get_algaebase_links.R")
end.time <- Sys.time()
runtime_algaebase <- round(end.time - start.time, 2)
```

```{r summary_algaebase, echo=FALSE}
# Print
cat(nrow(algaebase_results), "species and genera found in AlgaeBase")
```

# Extract culture information from NORCCA

```{r get_norcca, include=FALSE, message=FALSE, warning=FALSE}
start.time <- Sys.time()

norcca_compiler <- import_from_path("norcca_compiler", path = "src/python/")

strains_compiler <- norcca_compiler$StrainsCompiler()

strains_compiler$compile()

norcca <- do.call(rbind, lapply(strains_compiler$rows, 
                                function(x) as.data.frame(t(unlist(x)))))

write_delim(norcca, "data_in/norcca_strains.txt", 
            delim = "\t",
            na = "")

print(paste(nrow(norcca), "strains extracted"))
```

Add taxon_id to NORCCA strains and filter strains from Northern Europe

```{r wrangle_norcca, echo=FALSE}
if(!params$cache_norcca & file.exists("cache/norcca_cache.rda")) {
  temp <- file.remove("cache/norcca_cache.rda")
  temp <- file.remove("cache/norcca_nordic_cache.rda")
}

source("src/R/06_match_norcca_names.R")

# Print output
cat("Information from",
    length(unique(norcca_nordic$strain_name)),
    "strains extracted from NORCCA, matching",
    length(unique(norcca_nordic$scientific_name)),
    "NuA taxa")

end.time <- Sys.time()
runtime_get_norcca <- round(end.time - start.time, 2)
```

# Reformat IOC HAB list and add taxon_id
Downloaded (Text file, tab delimited) from
https://www.marinespecies.org/hab/aphia.php?p=download&what=taxlist

```{r get_hab, echo=FALSE, include=FALSE}
source("src/R/07_get_hab_links.R")
```

```{r summary_hab, echo=FALSE}
# Print
cat(nrow(nordic_hab), "IOC HAB species found in database")
```

# Get ITIS links
```{r get_itis, echo=FALSE, include=FALSE}
if(!params$cache_itis & file.exists("cache/itis_cache.rda")) {
  temp <- file.remove("cache/itis_cache.rda")
}

start.time <- Sys.time()
source("src/R/08_get_itis_links.R")
end.time <- Sys.time()
runtime_itis <- round(end.time - start.time, 2)
```

```{r summary_itis, echo=FALSE}
# Print
cat(nrow(itis_list), "taxa found in ITIS database")
```

# Get NCBI and ENA links
```{r get_ncbi, echo=FALSE, include=FALSE}
if(!params$cache_ncbi & file.exists("cache/ncbi_cache.rda")) {
  temp <- file.remove("cache/ncbi_cache.rda")
}

start.time <- Sys.time()
source("src/R/09_get_ncbi_links.R")
end.time <- Sys.time()
runtime_ncbi <- round(end.time - start.time, 2)
```

```{r summary_ncbi, echo=FALSE}
# Print
cat(nrow(ncbi_list), "taxa found in NCBI/ENA database")
```

# Get GBIF links and number of records
```{r get_gbif, echo=FALSE, include=FALSE}
if(!params$cache_gbif & file.exists("cache/gbif_cache.rda")) {
  temp <- file.remove("cache/gbif_cache.rda")
}

start.time <- Sys.time()
source("src/R/10_get_gbif_links.R")
end.time <- Sys.time()
runtime_gbif <- round(end.time - start.time, 2)
```

```{r summary_gbif, echo=FALSE}
# Print
cat(nrow(gbif_list), "taxa found in GBIF")
```

# Update filters.yaml for backend
Filters for Quick View may need to be updated, see output
```{r update_filters, echo=FALSE}
source("src/R/11_update_filters.R")
```

# Find taxa in SHARK
Download all phytoplankton observations from SHARK and find taxa missing from NuA database. Consider if these should be added to the database.
```{r get_shark_taxa, echo=FALSE, warning=FALSE, include=FALSE}
if(!params$cache_shark & file.exists("cache/shark_cache.rda")) {
  temp <- file.remove("cache/shark_cache.rda")
}

start.time <- Sys.time()
source("src/R/12_get_shark_taxa.R")
end.time <- Sys.time()
runtime_shark <- round(end.time - start.time, 2)

runtime_knit <- round(end.time - knit.time, 2)
```

```{r summary_shark, echo=FALSE}
# Print table
if(nrow(missing_shark_records) > 0){
  missing_shark_records %>%
  select(AphiaID,
         scientificname,
         status,
         rank,
         valid_AphiaID,
         valid_name) %>%
  knitr::kable(caption = "Phytoplankton taxa found in SHARK but missing in NuA database")
} else {
  cat("All SHARK phytoplankton taxa are already included in the NuA database")
}
```

# Summarise runtimes
```{r api_operation_summary, echo=FALSE}
runtime_variables <- c("knit",
                       "update_checklist",
                       "get_taxa_file",
                       "synonyms",
                       "match_dyntaxa",
                       "algaebase",
                       "get_norcca",
                       "itis",
                       "ncbi",
                       "gbif",
                       "shark")

runtime_values <- c(runtime_knit, runtime_update_checklist, runtime_get_taxa_file, runtime_synonyms, runtime_match_dyntaxa, runtime_algaebase, runtime_get_norcca, runtime_itis, runtime_ncbi, runtime_gbif, runtime_shark)

for (i in seq_along(runtime_variables)) {
  cat("Time taken for", runtime_variables[i], ": ", round(runtime_values[i]/3600, 2), "h", "\n")
}
```

# Create zip-archive
Store all output in update_history/update_archive_`r format(knit.time, "%Y%m%d")`.zip

```{r update_archive, echo=FALSE}
filename <- paste0('update_history/update_archive_', 
                          format(knit.time, "%Y%m%d"),
                          ".zip"
)

zip::zip(zipfile = filename, 
         files = c('data_out/backend/taxa/config/filters.yaml',
                   'data_out/content/facts_biovolumes_nomp.txt',
                   'data_out/content/facts_external_links_algaebase.txt',
                   'data_out/content/facts_external_links_dyntaxa.txt',
                   'data_out/content/facts_external_links_ena.txt',
                   'data_out/content/facts_external_links_gbif.txt',
                   'data_out/content/facts_external_links_hab_ioc.txt',
                   'data_out/content/facts_external_links_itis.txt',
                   'data_out/content/facts_external_links_ncbi.txt',
                   'data_out/content/facts_external_links_norcca.txt',
                   'data_out/content/facts_external_links_worms.txt',
                   'data_out/content/facts_hab_ioc_karlson_et_al_2021.txt',
                   'data_out/content/synonyms.txt',
                   'data_out/content/taxa.txt',
                   'data_out/duplicated_scientific_name.txt',
                   'data_out/taxa_worms_unaccepted.txt',
                   'data_out/taxa_worms.txt',
                   'data_out/translate_to_worms.txt',
                   paste0("data_out/nordicmicroalgae_checklist_", date, ".txt"),
                   paste0("data_out/nordicmicroalgae_checklist_", date, ".xlsx")
         )
)

cat("All output stored in", filename)
```

# Reproducibility

```{r reproducibility, echo=FALSE}
# Date time
cat("Time started:", format(knit.time))
cat("Time finished:", format(Sys.time()))

# Here we store the session info for this script
sessioninfo::session_info()
```